{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "import colour\n",
    "import random\n",
    "\n",
    "# Load the data from a CSV file.\n",
    "data = pd.read_csv('training.csv')\n",
    "\n",
    "# Split the data into input (X) and output (y).\n",
    "X = data.iloc[:, :3]\n",
    "y = data.iloc[:, 3:]\n",
    "\n",
    "# Split the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the input data.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the model architecture\n",
    "def create_model(hidden_size=10, num_layers=1, learning_rate=0.0001, dropout_rate=0.0, weight_decay=0.0, activation='relu', optimizer='Adam'):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(hidden_size, activation=activation, input_dim=3, kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    for _ in range(num_layers):\n",
    "        model.add(layers.Dense(hidden_size, activation=activation, kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(3))\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Use KerasRegressor wrapper\n",
    "model = KerasRegressor(model=create_model, epochs=100, batch_size=10, verbose=0, hidden_size=50, num_layers=1, learning_rate=0.0001, dropout_rate=0.0, weight_decay=0.0, activation='relu', optimizer='Adam')\n",
    "\n",
    "# Grid search hyperparameters\n",
    "hidden_size = [500, 600, 700]\n",
    "num_layers = [1, 2, 3, 4, 5]  # testing from 1 to 5 layers\n",
    "learning_rate = [0.0001, 0.001, 0.01]\n",
    "batch_size = [32, 64, 128]\n",
    "dropout_rate = [0.0, 0.05, 0.1, 0.2]\n",
    "weight_decay = [0.0, 0.01, 0.001]\n",
    "activations = ['relu']\n",
    "param_grid = dict(hidden_size=hidden_size, num_layers=num_layers, learning_rate=learning_rate, batch_size=batch_size, dropout_rate=dropout_rate, weight_decay=weight_decay, activation=activations)\n",
    "\n",
    "# Grid search with parallel execution\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "\n",
    "# Data Augmentation - adding Gaussian noise\n",
    "noise_std = 0.01\n",
    "noise = np.random.normal(0, noise_std, X_train_scaled.shape)\n",
    "X_train_scaled += noise\n",
    "\n",
    "# Split the training data into training and validation sets.\n",
    "X_train_scaled, X_val_scaled, y_train, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an ensemble of models. Each model in the ensemble is a neural network with the same architecture but different initial weights. \n",
    "models = []\n",
    "history = []  # to store history of each model\n",
    "for i in range(5):  # creating 5 models\n",
    "    model = create_model(hidden_size=grid_result.best_params_['hidden_size'], \n",
    "                         num_layers=grid_result.best_params_['num_layers'],\n",
    "                         learning_rate=grid_result.best_params_['learning_rate'], \n",
    "                         dropout_rate=grid_result.best_params_['dropout_rate'], \n",
    "                         weight_decay=grid_result.best_params_['weight_decay'], \n",
    "                         activation=grid_result.best_params_['activation'])\n",
    "    hist = model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), epochs=100, batch_size=grid_result.best_params_['batch_size'], verbose=0)\n",
    "    history.append(hist)\n",
    "    models.append(model)\n",
    "    # Save each model\n",
    "    model.save(f'model_{i}.h5')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Save the grid search results\n",
    "joblib.dump(grid_result.best_params_, 'best_params.pkl')\n",
    "\n",
    "# Plot the training and validation loss for each model\n",
    "for i, hist in enumerate(history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title(f'Model {i+1} loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.savefig(f'loss_{i+1}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Each model in the ensemble makes predictions on the test set.\n",
    "predictions = []\n",
    "for model in models:\n",
    "    predictions.append(model.predict(X_test_scaled))\n",
    "\n",
    "# Average the predictions of the ensemble to get the final prediction.\n",
    "y_pred_ensemble = np.mean(predictions, axis=0)\n",
    "\n",
    "# Visualize the predictions of the ensemble model\n",
    "plt.scatter(y_test, y_pred_ensemble)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Prediction by Ensemble Model')\n",
    "plt.grid(True)\n",
    "plt.savefig('scatter_predictions.png')\n",
    "plt.close()\n",
    "\n",
    "# Finally, calculate the mean squared error of the predictions. \n",
    "mse_ensemble = mean_squared_error(y_test, y_pred_ensemble)\n",
    "print(f\"Mean Squared Error of the ensemble model: {mse_ensemble}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
